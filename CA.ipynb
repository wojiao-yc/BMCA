{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fused_eeg_vit_co_processing.py\n",
    "# -------------------------------------------------------------\n",
    "# EEG (63x250) ↔ CLIP ViT-L/14 multi-stream co-processing\n",
    "# - Keeps the original CLIP ViT embedding frozen/unchanged for contrast\n",
    "# - Maps EEG → ViT token space (D=1024, L=256) via ATMS-style ts/sp conv + adapter\n",
    "# - Preserves your Medformer (as BrainEncoder) before the adapter\n",
    "# - Inserts bi-directional Cross-Attn bridges at several ViT blocks\n",
    "# - Outputs a gated fused embedding comparable to the frozen ViT embedding\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "# ====== Import your BrainEncoder bits (from ATMS.py) ======\n",
    "# Assumes ATMS.py is importable in PYTHONPATH; otherwise use relative import or sys.path.insert.\n",
    "from ATMS import Medformer, Config  # noqa\n",
    "\n",
    "# ==========================================================\n",
    "#                      LoRA helpers\n",
    "# ==========================================================\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r: int = 8, alpha: int = 16, bias: bool = False, freeze_main: bool = True):\n",
    "        super().__init__()\n",
    "        self.main = nn.Linear(in_features, out_features, bias=bias)\n",
    "        if freeze_main:\n",
    "            for p in self.main.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.r = r\n",
    "        if r > 0:\n",
    "            self.A = nn.Linear(in_features, r, bias=False)\n",
    "            self.B = nn.Linear(r, out_features, bias=False)\n",
    "            nn.init.kaiming_uniform_(self.A.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.B.weight)\n",
    "            self.scaling = alpha / r\n",
    "        else:\n",
    "            self.A = None\n",
    "            self.B = None\n",
    "            self.scaling = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.r > 0:\n",
    "            return self.main(x) + self.B(self.A(x)) * self.scaling\n",
    "        else:\n",
    "            return self.main(x)\n",
    "\n",
    "\n",
    "class CrossAttentionAdapter(nn.Module):\n",
    "    \"\"\"Bi-directional cross-attn building block used inside the bridge.\n",
    "\n",
    "    We expose a single-direction module here (Q from stream A, K/V from stream B),\n",
    "    and compose two of them in CoProcessingBridge.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int, d_kv: Optional[int] = None,\n",
    "                 lora_r: int = 8, lora_alpha: int = 16, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_kv = d_kv or (d_model // n_heads)\n",
    "        self.scale = self.d_kv ** -0.5\n",
    "\n",
    "        self.q = LoRALinear(d_model, n_heads * self.d_kv, r=lora_r, alpha=lora_alpha)\n",
    "        self.k = LoRALinear(d_model, n_heads * self.d_kv, r=lora_r, alpha=lora_alpha)\n",
    "        self.v = LoRALinear(d_model, n_heads * self.d_kv, r=lora_r, alpha=lora_alpha)\n",
    "        self.o = LoRALinear(n_heads * self.d_kv, d_model, r=lora_r, alpha=lora_alpha)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm_q = nn.LayerNorm(d_model)\n",
    "        self.gate = nn.Parameter(torch.zeros(1))  # learnable scalar gate\n",
    "\n",
    "    def forward(self, q_tokens: torch.Tensor, kv_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # q_tokens: [B, Lq, D], kv_tokens: [B, Lk, D]\n",
    "        B, Lq, D = q_tokens.shape\n",
    "        Lk = kv_tokens.shape[1]\n",
    "\n",
    "        qn = self.norm_q(q_tokens)\n",
    "        q = self.q(qn).view(B, Lq, self.n_heads, self.d_kv).transpose(1, 2)  # [B,H,Lq,d]\n",
    "        k = self.k(kv_tokens).view(B, Lk, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "        v = self.v(kv_tokens).view(B, Lk, self.n_heads, self.d_kv).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B,H,Lq,Lk]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        ctx = attn @ v  # [B,H,Lq,d]\n",
    "        ctx = ctx.transpose(1, 2).contiguous().view(B, Lq, self.n_heads * self.d_kv)\n",
    "        out = self.o(ctx)\n",
    "\n",
    "        # Residual with gated injection\n",
    "        out = q_tokens + torch.tanh(self.gate) * out\n",
    "        return out\n",
    "\n",
    "\n",
    "class CoProcessingBridge(nn.Module):\n",
    "    \"\"\"Projects both streams to shared dim, exchanges info both ways, and unprojects back.\"\"\"\n",
    "\n",
    "    def __init__(self, d_vit: int, d_brain: int, n_heads: int = 8, d_shared: Optional[int] = None,\n",
    "                 lora_r: int = 8, lora_alpha: int = 16, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.d_shared = d_shared or d_vit\n",
    "        self.v_proj = nn.Linear(d_vit, self.d_shared, bias=False)\n",
    "        self.b_proj = nn.Linear(d_brain, self.d_shared, bias=False)\n",
    "        self.v2b = CrossAttentionAdapter(self.d_shared, n_heads, lora_r=lora_r, lora_alpha=lora_alpha, dropout=dropout)\n",
    "        self.b2v = CrossAttentionAdapter(self.d_shared, n_heads, lora_r=lora_r, lora_alpha=lora_alpha, dropout=dropout)\n",
    "        self.v_unproj = nn.Linear(self.d_shared, d_vit, bias=False)\n",
    "        self.b_unproj = nn.Linear(self.d_shared, d_brain, bias=False)\n",
    "\n",
    "    def forward(self, vit_tokens: torch.Tensor, brain_tokens: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        V = self.v_proj(vit_tokens)\n",
    "        B = self.b_proj(brain_tokens)\n",
    "        V_new = self.b2v(V, B)\n",
    "        B_new = self.v2b(B, V)\n",
    "        vit_tokens = vit_tokens + self.v_unproj(V_new - V)\n",
    "        brain_tokens = brain_tokens + self.b_unproj(B_new - B)\n",
    "        return vit_tokens, brain_tokens\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#    EEG → ViT token adapter (ATMS-style + Medformer)\n",
    "# ==========================================================\n",
    "class BrainEncoderAdapter(nn.Module):\n",
    "    \"\"\"Use your Medformer to encode EEG, then adapt to ViT token grid (L=256, D=1024).\n",
    "\n",
    "    Input:  EEG [B, 63, 250]\n",
    "    Output: brain_tokens [B, 256, 1024]  (matches ViT-L/14 patch tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_len: int = 250, enc_in: int = 63, d_model_med: int = 250,\n",
    "                 vit_hidden: int = 1024, vit_token_len: int = 256, depth: int = 4):\n",
    "        super().__init__()\n",
    "        # Medformer as in your ATMS\n",
    "        cfg = Config(depth=depth)\n",
    "        cfg.seq_len = seq_len\n",
    "        cfg.enc_in = enc_in\n",
    "        cfg.d_model = d_model_med\n",
    "        self.medformer = Medformer(cfg)\n",
    "\n",
    "        # Project Medformer features → ViT hidden\n",
    "        self.to_vit_hidden = nn.Linear(d_model_med, vit_hidden)\n",
    "\n",
    "        # Token count adapter:  variable L → fixed 256 via 1D interpolation in token axis\n",
    "        self.vit_token_len = vit_token_len\n",
    "        self.norm = nn.LayerNorm(vit_hidden)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _length_to_256(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, D] → [B, 256, D]\n",
    "        B, L, D = x.shape\n",
    "        if L == self.vit_token_len:\n",
    "            return x\n",
    "        x_t = x.transpose(1, 2)  # [B, D, L]\n",
    "        x_t = F.interpolate(x_t, size=self.vit_token_len, mode=\"linear\", align_corners=False)\n",
    "        x = x_t.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, eeg: torch.Tensor) -> torch.Tensor:\n",
    "        # eeg: [B, 63, 250]\n",
    "        # Medformer expects (B, C, T) → patch-embedded → encoder → features\n",
    "        feats = self.medformer(eeg)              # shape ~ [B, Lm, d_model_med] per ATMS.Medformer.classification\n",
    "        feats = self.to_vit_hidden(feats)        # [B, Lm, 1024]\n",
    "        feats = self._length_to_256(feats)       # [B, 256, 1024]\n",
    "        feats = self.norm(feats)\n",
    "        return feats\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#       Fused model that runs CLIP ViT with bridges\n",
    "# ==========================================================\n",
    "class FusedEEGViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 insert_layers: Iterable[int] = (6, 12, 18),\n",
    "                 n_heads_bridge: int = 8,\n",
    "                 lora_r: int = 8,\n",
    "                 med_depth: int = 4,\n",
    "                 vit_model_name: str = 'openai/clip-vit-large-patch14'):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Load frozen CLIP ViT-L/14\n",
    "        self.clip = CLIPVisionModel.from_pretrained(vit_model_name)\n",
    "        for p in self.clip.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.clip.eval()\n",
    "\n",
    "        self.vit_hidden = self.clip.config.hidden_size  # 1024 for ViT-L/14\n",
    "        self.vit_token_len = (self.clip.config.image_size // self.clip.config.patch_size) ** 2  # 16*16=256\n",
    "\n",
    "        # 2) Brain side (Medformer + adapter to ViT token space)\n",
    "        self.brain_adapter = BrainEncoderAdapter(seq_len=250, enc_in=63,\n",
    "                                                 d_model_med=250, vit_hidden=self.vit_hidden,\n",
    "                                                 vit_token_len=self.vit_token_len, depth=med_depth)\n",
    "\n",
    "        # 3) Multi-spot bridges\n",
    "        self.insert_layers = set(insert_layers)\n",
    "        self.bridges = nn.ModuleDict()\n",
    "        for idx in insert_layers:\n",
    "            self.bridges[str(idx)] = CoProcessingBridge(d_vit=self.vit_hidden, d_brain=self.vit_hidden,\n",
    "                                                        n_heads=n_heads_bridge, lora_r=lora_r, lora_alpha=16,\n",
    "                                                        dropout=0.0)\n",
    "\n",
    "        # 4) Gating for final fused embedding vs original CLIP embedding\n",
    "        self.gate_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.vit_hidden),\n",
    "            nn.Linear(self.vit_hidden, self.vit_hidden // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.vit_hidden // 4, 1),\n",
    "            nn.Sigmoid(),  # per-sample scalar g in [0,1]\n",
    "        )\n",
    "\n",
    "        # A small projection to CLIP projection space (if you need to compare with pooled output)\n",
    "        # Here we emulate CLIP's final pooling by taking CLS then applying the same post_layernorm\n",
    "        self.final_norm = nn.LayerNorm(self.vit_hidden)\n",
    "\n",
    "    def vit_embed_tokens(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run patch embedding + positional + pre layer norm to get initial tokens.\n",
    "        Returns tokens before encoder blocks. Shape: [B, 1+256, 1024]\n",
    "        \"\"\"\n",
    "        vm = self.clip.vision_model\n",
    "        x = vm.embeddings.patch_embedding(pixel_values)  # [B, D, 16, 16]\n",
    "        x = x.flatten(2).transpose(1, 2)                 # [B, 256, D]\n",
    "        cls = vm.embeddings.class_embedding.to(x.dtype).unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)                   # [B, 257, D]\n",
    "        pos = vm.embeddings.position_embedding(vm.embeddings.position_ids[:, : x.size(1)])\n",
    "        x = x + pos\n",
    "        x = vm.pre_layrnorm(x)\n",
    "        return x\n",
    "\n",
    "    def run_encoder_with_bridges(self, tokens: torch.Tensor, brain_tokens: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward through ViT encoder, injecting bridges after selected blocks.\n",
    "        tokens: [B, 1+256, D], brain_tokens: [B, 256, D]\n",
    "        Returns updated (tokens, brain_tokens)\n",
    "        \"\"\"\n",
    "        enc = self.clip.vision_model.encoder\n",
    "        for i, blk in enumerate(enc.layers):\n",
    "            # standard ViT block forward (frozen)\n",
    "            tokens = blk(tokens)[0]  # Transformers blocks may return BaseModelOutputWithPast\n",
    "            if (i + 1) in self.insert_layers:\n",
    "                # exclude class token for cross-attn (optional): only patch tokens interact\n",
    "                cls_tok, patch_tok = tokens[:, :1, :], tokens[:, 1:, :]\n",
    "                bridge = self.bridges[str(i + 1)]\n",
    "                patch_tok, brain_tokens = bridge(patch_tok, brain_tokens)\n",
    "                tokens = torch.cat([cls_tok, patch_tok], dim=1)\n",
    "        tokens = enc.post_layernorm(tokens)\n",
    "        return tokens, brain_tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def _cls_pool(tokens: torch.Tensor) -> torch.Tensor:\n",
    "        return tokens[:, 0]  # [B, D]\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, eeg: torch.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        pixel_values: [B,3,224,224] (already normalized as CLIP expects)\n",
    "        eeg: [B,63,250]\n",
    "        Returns a dict with original CLIP embedding, fused embedding, and gating value.\n",
    "        \"\"\"\n",
    "        # 1) Original (pure) CLIP embedding — frozen, used as contrast target\n",
    "        with torch.no_grad():\n",
    "            pure_outputs = self.clip(pixel_values)\n",
    "            pure_tokens = pure_outputs.last_hidden_state  # [B, 257, D]\n",
    "            pure_tokens = self.clip.vision_model.post_layernorm(pure_tokens)\n",
    "            z_pure = self._cls_pool(pure_tokens)  # [B, D]\n",
    "\n",
    "        # 2) EEG → ViT token space\n",
    "        brain_tokens = self.brain_adapter(eeg)  # [B, 256, D]\n",
    "\n",
    "        # 3) ViT tokens before encoder\n",
    "        vit_tokens = self.vit_embed_tokens(pixel_values)\n",
    "\n",
    "        # 4) Run encoder with bridges (multi-exchange)\n",
    "        vit_tokens, brain_tokens = self.run_encoder_with_bridges(vit_tokens, brain_tokens)\n",
    "\n",
    "        # 5) Fused embedding from class token\n",
    "        z_fused = self._cls_pool(vit_tokens)     # [B, D]\n",
    "        z_fused = self.final_norm(z_fused)\n",
    "\n",
    "        # 6) Learn a gate g ∈ [0,1] per sample (can be conditioned on EEG-aware CLS)\n",
    "        g = self.gate_head(z_fused).squeeze(-1)  # [B]\n",
    "        z_gate = g.unsqueeze(-1) * z_fused + (1.0 - g).unsqueeze(-1) * z_pure\n",
    "\n",
    "        return {\n",
    "            'z_pure': z_pure.detach(),      # frozen CLIP embedding (no grad)\n",
    "            'z_fused': z_fused,             # fused (with bridges) embedding\n",
    "            'z_gate': z_gate,               # gated final embedding (for contrastive / retrieval)\n",
    "            'gate': g,                      # [B]\n",
    "        }\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#                 Losses / Training helpers\n",
    "# ==========================================================\n",
    "class InfoNCE(nn.Module):\n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * math.log(1.0 / temperature))\n",
    "\n",
    "    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:\n",
    "        # z_a, z_b: [B, D]\n",
    "        z_a = F.normalize(z_a, dim=-1)\n",
    "        z_b = F.normalize(z_b, dim=-1)\n",
    "        logits = z_a @ z_b.t() * self.logit_scale.exp()\n",
    "        labels = torch.arange(z_a.size(0), device=z_a.device)\n",
    "        loss = F.cross_entropy(logits, labels) + F.cross_entropy(logits.t(), labels)\n",
    "        return loss / 2\n",
    "\n",
    "\n",
    "def contrastive_step(model: FusedEEGViT, batch: dict, temperature: float = 0.07,\n",
    "                     lambda_sync: float = 0.1) -> Tuple[torch.Tensor, dict]:\n",
    "    \"\"\"One training step example.\n",
    "\n",
    "    batch must contain:\n",
    "      - pixel_values: [B,3,224,224] (CLIP preprocessed)\n",
    "      - eeg: [B,63,250]\n",
    "    \"\"\"\n",
    "    outputs = model(batch['pixel_values'], batch['eeg'])\n",
    "\n",
    "    z_pure = outputs['z_pure']\n",
    "    z_gate = outputs['z_gate']\n",
    "\n",
    "    # Contrastive loss: bring (z_gate) close to (z_pure) to preserve CLIP semantics\n",
    "    nce = InfoNCE(temperature)\n",
    "    loss_contrast = nce(z_gate, z_pure)\n",
    "\n",
    "    # Optional: simple sync regularizer aligning patch means\n",
    "    # (You can craft a richer sync using intermediate tokens if you expose them.)\n",
    "    # Here, for simplicity, we use cosine between fused and pure CLS\n",
    "    loss_sync = 1.0 - F.cosine_similarity(z_gate, z_pure, dim=-1).mean()\n",
    "\n",
    "    loss = loss_contrast + lambda_sync * loss_sync\n",
    "    log = {\n",
    "        'loss_contrast': loss_contrast.item(),\n",
    "        'loss_sync': loss_sync.item(),\n",
    "        'gate_mean': outputs['gate'].mean().item(),\n",
    "    }\n",
    "    return loss, log\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#            How to unfreeze only what we need\n",
    "# ==========================================================\n",
    "def set_trainable_parameters(model: FusedEEGViT):\n",
    "    \"\"\"Freeze all CLIP params; train only bridges + gate + brain adapter (and Medformer tail if desired).\"\"\"\n",
    "    # 1) Keep CLIP frozen (already requires_grad=False)\n",
    "\n",
    "    # 2) Enable training for bridges (LoRA params) and gate\n",
    "    for m in [model.bridges, model.gate_head, model.final_norm]:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # 3) Brain side: either fully or partially trainable\n",
    "    for name, p in model.brain_adapter.named_parameters():\n",
    "        # Example: freeze early Medformer layers by name rule if needed\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#                       Usage sketch\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = FusedEEGViT(insert_layers=(6, 12, 18), n_heads_bridge=8, lora_r=8).to(device)\n",
    "    set_trainable_parameters(model)\n",
    "\n",
    "    B = 2\n",
    "    pixel_values = torch.randn(B, 3, 224, 224, device=device)\n",
    "    eeg = torch.randn(B, 63, 250, device=device)\n",
    "\n",
    "    out = model(pixel_values, eeg)\n",
    "    for k, v in out.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            print(k, tuple(v.shape))\n",
    "        else:\n",
    "            print(k, v)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

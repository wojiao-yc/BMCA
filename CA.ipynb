{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements: torch, transformers, einops (可选)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "# -------------------------\n",
    "# 基本构件：跨流 CrossAttention\n",
    "# -------------------------\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "\n",
    "        self.q = nn.Linear(dim, dim, bias=False)\n",
    "        self.k = nn.Linear(dim, dim, bias=False)\n",
    "        self.v = nn.Linear(dim, dim, bias=False)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key_value, attn_mask=None):\n",
    "        # query: [B, Tq, D], key_value: [B, Tk, D]\n",
    "        B, Tq, D = query.shape\n",
    "        Tk = key_value.shape[1]\n",
    "        q = self.q(query).view(B, Tq, self.n_heads, self.head_dim).transpose(1,2)  # [B, H, Tq, Hd]\n",
    "        k = self.k(key_value).view(B, Tk, self.n_heads, self.head_dim).transpose(1,2)  # [B, H, Tk, Hd]\n",
    "        v = self.v(key_value).view(B, Tk, self.n_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / (self.head_dim ** 0.5)  # [B, H, Tq, Tk]\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)  # [B, H, Tq, Hd]\n",
    "        out = out.transpose(1,2).contiguous().view(B, Tq, D)\n",
    "        return self.out(out), attn  # 返回注意力以便可视化\n",
    "\n",
    "# -------------------------\n",
    "# Brain Transformer Block（带 CrossFromModel）\n",
    "# -------------------------\n",
    "class BrainBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, batch_first=True)\n",
    "        self.cross_from_model = CrossAttention(dim=dim, n_heads=n_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, int(dim*mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(dim*mlp_ratio), dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, model_kv=None, attn_mask=None):\n",
    "        # x: [B, T_b, D_b]; model_kv: [B, T_m, D_b] (projected if needed)\n",
    "        x2, _ = self.self_attn(x, x, x)\n",
    "        x = x + x2\n",
    "        x = self.norm1(x)\n",
    "        if model_kv is not None:\n",
    "            cross_out, attn = self.cross_from_model(x, model_kv, attn_mask=attn_mask)\n",
    "            x = x + cross_out\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.ffn(self.norm3(x))\n",
    "        return x, attn if model_kv is not None else None\n",
    "\n",
    "# -------------------------\n",
    "# Model Stream wrapper (pretrained LM with insertion point)\n",
    "# -------------------------\n",
    "class LMWithCross(nn.Module):\n",
    "    def __init__(self, lm_name='gpt2', cross_dim=512, insert_layers=[4,8,12]):\n",
    "        super().__init__()\n",
    "        # load a pretrained model and expose hidden states\n",
    "        self.config = AutoConfig.from_pretrained(lm_name, output_hidden_states=True)\n",
    "        self.lm = AutoModelForCausalLM.from_pretrained(lm_name, config=self.config)\n",
    "        self.insert_layers = insert_layers\n",
    "        # project brain dim to LM hidden dim if necessary\n",
    "        self.brain_to_lm_proj = nn.Linear(cross_dim, self.config.hidden_size)\n",
    "        # create CrossAttention modules to query brain stream from LM stream\n",
    "        self.cross_modules = nn.ModuleDict({\n",
    "            str(i): CrossAttention(self.config.hidden_size, n_heads=8) for i in insert_layers\n",
    "        })\n",
    "\n",
    "    def forward(self, input_ids, brain_repr_by_layer=None, attention_mask=None):\n",
    "        # brain_repr_by_layer: dict(layer_idx -> tensor [B, T_b, D_b])\n",
    "        outputs = self.lm.model(input_ids, output_hidden_states=True, attention_mask=attention_mask)\n",
    "        hs = list(outputs.hidden_states)  # tuple of (embeddings, layer1, layer2,...)\n",
    "        # iterate through layers and inject cross-attn where desired (simplified)\n",
    "        attn_maps = {}\n",
    "        for idx in self.insert_layers:\n",
    "            layer_h = hs[idx]  # [B, T_m, H]\n",
    "            if brain_repr_by_layer and idx in brain_repr_by_layer:\n",
    "                br = brain_repr_by_layer[idx]  # [B, T_b, D_b]\n",
    "                br_proj = self.brain_to_lm_proj(br)  # [B, T_b, H]\n",
    "                cross_out, attn = self.cross_modules[str(idx)](layer_h, br_proj)\n",
    "                # residual add (simplified — real injection needs to integrate into transformer block)\n",
    "                hs[idx] = layer_h + cross_out\n",
    "                attn_maps[idx] = attn\n",
    "        # compute logits by passing last hidden state through LM head\n",
    "        last_h = hs[-1]\n",
    "        logits = self.lm.lm_head(last_h)\n",
    "        return logits, attn_maps\n",
    "\n",
    "# -------------------------\n",
    "# 顶层并行模型组合\n",
    "# -------------------------\n",
    "class CoProcessingModel(nn.Module):\n",
    "    def __init__(self, brain_dim=512, n_brain_layers=6, lm_name='gpt2', insert_layers=[4,8]):\n",
    "        super().__init__()\n",
    "        self.brain_embed = nn.Linear(in_features=64, out_features=brain_dim)  # 假设原始脑特征维64\n",
    "        self.brain_blocks = nn.ModuleList([BrainBlock(brain_dim, n_heads=8) for _ in range(n_brain_layers)])\n",
    "        self.lm_with_cross = LMWithCross(lm_name=lm_name, cross_dim=brain_dim, insert_layers=insert_layers)\n",
    "\n",
    "    def forward(self, brain_inputs, input_ids, brain_time_to_layer_map=None):\n",
    "        # brain_inputs: [B, T_b, feat_dim]\n",
    "        x = self.brain_embed(brain_inputs)  # project\n",
    "        brain_by_layer = {}\n",
    "        # iterate brain layers, optionally cross from LM hidden states via brain_time_to_layer_map\n",
    "        for i, blk in enumerate(self.brain_blocks):\n",
    "            # optionally obtain model hidden to cross from (here ignored for simplicity)\n",
    "            x, attn = blk(x, model_kv=None)\n",
    "            brain_by_layer[i] = x\n",
    "        # map brain layers to LM insert layer indices (simple mapping or learned)\n",
    "        mapped = {}\n",
    "        for lm_idx in self.lm_with_cross.insert_layers:\n",
    "            # simple mapping: choose brain layer by index mod\n",
    "            b_idx = lm_idx % len(self.brain_blocks)\n",
    "            mapped[lm_idx] = brain_by_layer[b_idx]\n",
    "        logits, attn_maps = self.lm_with_cross(input_ids, brain_repr_by_layer=mapped)\n",
    "        return logits, attn_maps\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
